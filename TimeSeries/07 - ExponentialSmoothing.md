## Exponential Smoothing

Exponential smoothing was proposed in the late 1950s and has motivated some of the most successful forecasting methods. Forecasts produced using exponential smoothing
methods are weighted averages of past observations, with the weights decaying exponentially as the observations get older. In other words, the more recent the
observation the higher the associated weight.

In the first part, we present the mechanics of the most important exponential smoothing methods and their application in forecasting time series with various
characteristics. In the second part, we present the statistical models that underlie exponential smoothing methods.

## Simple exponential smoothing
The simplest of the exponentially smoothing methods is naturally called *simple exponential smoothing (SES)*. This method is suitable for forecasting data with no
clear trend or seasonal pattern.

Using the naïve method, all forecasts for the future are equal to the last observed value of the series. Hence, the naïve method assumes that the most recent observation
is the only important one and all previous observations provide no information for the future.

Using the average method, all future forecasts are equal to a simple average of the observed data. Hence, the average method assumes that all observations are of equal
importance and gives them equal weights when generating forecasts.

We often want something between these two extremes. For example, it may be sensible to attach larger weights to more recent observations than to observations from
the distant past. This is exacctly the concept behind simple exponential smoothing. Forecasts are calculated using weighted averages, where the weights decrease
exponentially as observations come from further in the past - the smallest weights are associated with the oldest observations.
#### Weighted average form
The forecast at time T + 1 is equal to a weighted average between the most recent observation yT and the previous forecast y(T|T-1).
#### Component form
An alternative representation is the component form. For simple exponential smoothing, the only component included is the level, lt. Other methods may also included a
trend bt and a seasonal component st. Component form representations of exponential smoothing methods comprise a forecast equation and a smoothing equation for each
of the components included in the method. The component form of simple exponential smoothing is given by:

![equation](https://github.com/gpadolina/TimeSeries-notes/blob/master/TimeSeries/Equations/Component%20form.png)

The component form of simple exponential smoothing is not particularly useful, but it will be the easiest form to use when we start adding other components.
#### Flat forecasts
Simple exponental smoothing has a "flat" forecast function:

![equation](https://github.com/gpadolina/TimeSeries-notes/blob/master/TimeSeries/Equations/Flat%20forecasts.png)

That is, all forecasts take the same value, equal to the last level component. Remember that these forecasts
will only be suitable if the time series has no trend or seasonal component.
#### Optimization
The application of every exponential smoothing method requires the smoothing parameters and the inital values to be chosen. In particular, for simple exponential
smoothing, we need to select the values of alpha and l0. All forecasts can be computed from the data once we know those values.

In some cases, the smoothing parameters may be chosen in a subjective manner. However, a more reliable and objective way to obtain values for the unknown parameters is
to estimate them from the observed data.

Regression model coefficients can be estimated by minimizing the sum of the squared residuals. Similarly, the unknown parameters and the initial values for any
exponential smoothing method can be estimated by minimizing the SSE.

Unlike regression where we have formulas which return the values of the regression coefficients that minimizes the SSE, this involves a non-linear minimization problem
and we need to use an optimization tool to solve it.
## Trend methods
#### Holt's linear trend method
Holt extended simple exponential smoothing to allow the forecasting of data with a trend. This method involves a forecast equation and two smoothing equations, one for
the level and one for the trend:

![equation](https://github.com/gpadolina/TimeSeries-notes/blob/master/TimeSeries/Equations/Holt's%20linear%20trend%20method.png)

```
air <- window(ausair, start=1990) 
fc <- holt(air, h=5)
```
The forecast function is no longer flat but trending. The h-step-ahead forecast is equal to the last estimated level plus h times the last estimated trend value. Hence,
the forecasts are a linear function of h.
#### Damped trend methods
The forecasts generated by Holt's linear method display a constant trend (increasing or decreasing) indefinitely into the future. Empirical evidence indicates that these
methods tend to over-forecast, especially for longer forecast horizons. Motivated by this observation, Gardner & McKenzie introduced a parameter that "dampens" the
trend to a flat line some time in the future. Methods that include a damped trend have proven to be very successful and are arguably the most popular individual
methods when forecasts are required automatically for many series.

In conjunction with the smoothing parameters alpha and beta, with values between 0 and 1 as in Holt's method, this method also includes a damping parameter 0 < phi < 1:

![equation](https://github.com/gpadolina/TimeSeries-notes/blob/master/TimeSeries/Equations/Damped%20trend%20methods.png)

```
fc <- holt(air, h=15)
fc2 <- holt(air, damped=TRUE, phi = 0.9, h=15) 
autoplot(air) +
  autolayer(fc, series="Holt's method", PI=FALSE) + 
  autolayer(fc2, series="Damped Holt's method", PI=FALSE) + 
  ggtitle("Forecasts from Holt's method") + xlab("Year") + 
  ylab("Air passengers in Australia (millions)") + 
  guides(colour=guide_legend(title="Forecast"))
```
Damped Holt's method is best whether you compare MAE or MSE values.
```
fc <- holt(livestock, damped=TRUE) 
# Estimated parameters: 
fc[["model"]]
```
## Holt-Winters' seasonal method
Holt and Winters extended Holt's method to capture seasonality. The Holt-Winters seasonal method comprises the forecast equation and three smoothing equations - one
for the level lt, one for the trend bt, and one for the seaonsal component st, with corresponding smoothing parameters alpha, beta, and phi. We use m to denote the
frequency of the seasonality, the number of seasons in a year.

There are two variations to this method that differs in the nature of the seasonal component. The additive method is preferred when the seasonal variations are
roughly constant through the series, while the multiplicative method is preferred when the seasonal variations are changing proportional to the level of the series.
#### Holt-Winters' additive method
The component form for the additive method is:

![equation](https://github.com/gpadolina/TimeSeries-notes/blob/master/TimeSeries/Equations/Holt-Winters'%20additive%20method.png)
#### Holt-Winters' multiplicative method
The component form for the multiplicative method is:

![equation](https://github.com/gpadolina/TimeSeries-notes/blob/master/TimeSeries/Equations/Holt-Winters'%20multiplicative%20method.png)

#### Example:
```
aust <- window(austourists,start=2005) 
fit1 <- hw(aust,seasonal="additive")
fit2 <- hw(aust,seasonal="multiplicative") 
autoplot(aust) +
  autolayer(fit1, series="HW additive forecasts", PI=FALSE) + 
  autolayer(fit2, series="HW multiplicative forecasts", PI=FALSE) +
  xlab("Year") +
  ylab("Visitor nights (millions)") + 
  ggtitle("International visitors nights in Australia") + 
  guides(colour=guide_legend(title="Forecast"))
```
#### Holt-Winters' damped method
Damping is possible with both additive and multiplicative Holt-Winters' methods. A method that often provides accurate and robust forecasts for seasonal data is
the Holt-Winters method with a damped trend and multiplicative seasonality:

![equation](https://github.com/gpadolina/TimeSeries-notes/blob/master/TimeSeries/Equations/Holt-Winters'%20damped%20method.png)
```
hw(y, damped=TRUE, seasonal="multiplicative")
```
#### Example: Holt-Winters method
```
 fc <- hw(subset(hyndsight,end=length(hyndsight)-35), 
  damped = TRUE, seasonal="multiplicative", h=35)
autoplot(hyndsight) +
  autolayer(fc, series="HW multi damped", PI=FALSE)+ 
  guides(colour=guide_legend(title="Daily forecasts"))
```
## A taxonomy of exponential smoothing methods
Exponential smoothing methods are not restricted to those presented so far. By considering variation in the combinations of the rend and seasonal components, nine
exponential methods are possible.

![equation](https://github.com/gpadolina/TimeSeries-notes/blob/master/TimeSeries/Equations/Two-way%20classical%20of%20exponential%20smoothing.png)

![equation](https://github.com/gpadolina/TimeSeries-notes/blob/master/TimeSeries/Equations/Two-way%20classification%20of%20exponential%20smoothing%20-%202.png)

Recursive formulas for appyling the nine exponential smoothing methods:

![equation](https://github.com/gpadolina/TimeSeries-notes/blob/master/TimeSeries/Equations/Recursive%20formulas%20for%20nine%20exponential%20smoothing%20methods.png)

## Innovations state space models for exponential smoothing
The exponential smoothing methods presented are algorithms which generate point forecasts. The statistical models in this section generate the same point forecasts,
but can also generate prediction or forecast intervals. A statistical model is a stochastic or random data generating process that can produce an entire forecast
distribution.

Each model consists of a measurement equation that describes the observed data and some state equations that describe how the unobserved components or states
(level, trend, seasonal) change over time. Hence, these are referred to as *state space models*.

For each method there exist two models: one with additive errors and one with multiplicative erros. The point forecasts producded by the models are identical if
they use the same smoothing parameter values. They will, however, generate different prediction intervals.

To distinguish between a model with additive errors and one with multiplicative errors, we add a third letter to the classification table. We label each state space
model as ETS( ) for (Error, Trend, Seasonal). This label can also be thought of as ExponenTial Smoothing.
#### EST(A,N,N): simple exponential smoothing with additive errors
Recall the component form of simple exponential smoothing:

![equation](https://github.com/gpadolina/TimeSeries-notes/blob/master/TimeSeries/Equations/Component%20form.png)

If we re-arrange the smoothing equation for the level, we get the "error correction" form:

![equation](https://github.com/gpadolina/TimeSeries-notes/blob/master/TimeSeries/Equations/Error%20correction%20form.png)

#### ETS(M,N,N): simple exponential smoothing with multiplicative errors
We can specifiy models with multiplicative errors by writing the one-step-ahead training errors as relative errors:

![equation](https://github.com/gpadolina/TimeSeries-notes/blob/master/TimeSeries/Equations/Relative%20errors.png)

Then we can write the multiplicative form of the state space model as

![equation](https://github.com/gpadolina/TimeSeries-notes/blob/master/TimeSeries/Equations/State%20space%20model%20multiplicative%20form.png)
